{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debuggin neural networks can be very difficult. Pytorch lightning provides a few tools to help with this.\n",
    "\n",
    "Some useful ones (among others) that will be explored are the trainer flags:\n",
    " - fast_dev_run=True (this will not save checkpoints or log anything, in stead this will \"touch\" every part of your code from training to validation to test and just make sure it runs)\n",
    " - overfit_batch=int|float (this will take a single batch of your training data and train on it over and over in an attempt to overfit on it. If the model is not able to overfit on a single batch of data, then there is likely a problem with the model) set it to an iteger (e.g. 1) then it will use one batch. You can also set it to a float (e.g. 0.1) and it will use 10% of the data.\n",
    " - num_sanity_val_steps=int (this will run the validation loop on the first n batches of the validation set. This is useful to make sure that the validation loop is working as expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a breakpoint in the code using the following command:\n",
    "    import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "def generate_cont_xor_data(num_points):\n",
    "    \"\"\"Generate a random XOR dataset with two continuous features.\"\"\"\n",
    "    x = torch.rand(num_points, 2)\n",
    "    y = torch.logical_xor(x[:, 0] > 0.5, x[:, 1] > 0.5).long()\n",
    "    return x, y\n",
    "\n",
    "x, y = generate_cont_xor_data(1000)\n",
    "\n",
    "class XORDataset(Dataset):\n",
    "    def __init__(self, num_points):\n",
    "        self.x, self.y = generate_cont_xor_data(num_points)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"point\": self.x[index],\n",
    "            \"label\": self.y[index]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "loader = DataLoader(XORDataset(10000), batch_size=32)\n",
    "for item in loader:\n",
    "    points, labels = item[\"point\"], item[\"label\"]\n",
    "    print(points.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a simple model so we can see the debuggin process in action\n",
    "\n",
    "class XORModel(pl.LightningModule):\n",
    "    \"\"\"This model predicts the XOR of two real inputs (continuous XOR)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
